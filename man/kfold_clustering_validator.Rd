% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/robustness.R
\name{kfold_clustering_validator}
\alias{kfold_clustering_validator}
\title{K-Fold Clustering Validator}
\usage{
kfold_clustering_validator(
  data,
  clustering_algo,
  labels_name,
  k_folds = 5,
  n_runs = 30,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{data}{A matrix or data frame of shape \code{(n_samples, n_features)}.}

\item{clustering_algo}{A function or callable (e.g. \code{kmeans}) that performs clustering
and is compatible with \code{\link{stochastic_clustering_runner}}.}

\item{labels_name}{A string with the name of the label containing the clusters returned by the algorithm.}

\item{k_folds}{Integer. Number of folds (default 5).}

\item{n_runs}{Integer. Number of repeated runs for each clustering call (default 30).}

\item{verbose}{Logical. If \code{TRUE}, prints progress messages (default \code{FALSE}).}

\item{...}{Additional parameters passed on to \code{\link{stochastic_clustering_runner}} and
subsequently to \code{clustering_algo}.}
}
\value{
A named list with:
\describe{
  \item{\code{baseline_results}}{
    The result of clustering the full dataset (a list typically containing elements such as
    \code{majority_voting_labels}, \code{ecc}, \code{partitions}, etc.).}
  \item{\code{kfolds_robustness_results}}{
    A list with one entry per fold. Each entry is itself a list containing:
    \describe{
      \item{\code{stochastic_clustering_results}}{The clustering result on the subset of data (fold).}
      \item{\code{used_indices}}{Indices of the data rows used for this fold’s clustering.}
      \item{\code{leave_out_indices}}{Indices of the data rows left out in this fold.}
      \item{\code{el_score_vector}}{A numeric vector of element-wise similarity scores comparing
            the fold’s majority-voting labels to the baseline’s labels on the same indices,
            as computed by \code{clustassess::element_sim_elscore}.}
    }}
}
}
\description{
Perform k-fold validation on a clustering algorithm by splitting the data into k folds.
On each iteration, the algorithm clusters the "training" fold (k-1 parts), and we compare
the resulting majority-voting labels to the baseline clustering labels (obtained from clustering
the entire dataset).
}
